{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89883948-6eb5-45ab-b5cf-17d4735525bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Product Data into Bronze Layer in Databricks\n",
    "\n",
    "1. Load the product JSON file from the specified path into a DataFrame.\n",
    "2. Add an `ingestion_timestamp` column to the DataFrame.\n",
    "3. Append the DataFrame to the Delta table named `bronze_products` in the `globalretail_bronze` database.\n",
    "4. Move the processed JSON file to an archive folder with a unique timestamped name.\n",
    "\n",
    "This process ensures that the product data is loaded efficiently and incrementally into the bronze layer, with data lineage and auditability preserved.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c291e8-af67-4dfd-965f-28723f5bf91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads a Parquet file from the specified DBFS path into a Spark DataFrame.\n",
    "\n",
    "Parameters:\n",
    "- filePath: str\n",
    "    The path to the Parquet file in Databricks File System (DBFS).\n",
    "\n",
    "Returns:\n",
    "- df: pyspark.sql.DataFrame\n",
    "    The resulting Spark DataFrame containing the Parquet data.\n",
    "\"\"\"\n",
    "\n",
    "filePath = \"dbfs:/FileStore/GlobalRetail/bronze_layer/transaction/transaction_snappy.parquet\"\n",
    "df = spark.read.parquet(filePath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7e6b06-0bdd-4647-b6e8-88700b21b53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts the 'transaction_date' column in the DataFrame 'df' to a timestamp type using to_timestamp,\n",
    "and displays the resulting DataFrame.\n",
    "\n",
    "- 'col': Refers to the column in the DataFrame to be transformed.\n",
    "- 'to_timestamp': Converts a string or numeric column to timestamp type.\n",
    "\n",
    "Returns:\n",
    "- new_df: DataFrame with 'transaction_date' as a timestamp column.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "new_df = df.withColumn(\"transaction_date\", to_timestamp(col(\"transaction_date\")))\n",
    "new_df.printSchema()\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5311dbc5-c59f-4ce5-8ec2-9c0cd5ed04ad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766513746455}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adds an 'ingestion_timestamp' column with the current timestamp to the DataFrame 'new_df'\n",
    "and displays the resulting DataFrame.\n",
    "\n",
    "Note: new_df.withColumn returns a new DataFrame; the original 'new_df' remains unchanged.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "final_df = new_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33521142-3c30-43d2-b8c0-fedfb501fc8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Appends the DataFrame 'final_df' to the 'bronze_transactions' Delta table in the 'globalretail_bronze' database.\n",
    "Uses Delta format for ACID transactions, scalable metadata handling, and unified streaming/batch data processing.\n",
    "\"\"\"\n",
    "spark.sql(\"use globalretail_bronze\")\n",
    "final_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b17729-4a27-4b8d-a270-6752b75477e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Queries the 'bronze_transactions' Delta table and returns the first 100 rows.\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(\"select * from bronze_transactions limit 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf58dfe-23d0-4d77-8418-d9657f0325de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Moves the transaction Parquet file to an archive folder in DBFS with a unique timestamped name.\n",
    "\n",
    "Archiving the ingested file ensures data lineage, auditability, and recovery by preserving the original raw data.\n",
    "This prevents accidental reprocessing, supports compliance, and enables troubleshooting or re-ingestion if needed.\n",
    "\n",
    "Prints the archive file path for logging.\n",
    "\"\"\"\n",
    "import datetime\n",
    "archive_folder = \"dbfs:/FileStore/GlobalRetail/bronze_layer/transaction/archive/\"\n",
    "archive_filepath = archive_folder +'_'+datetime.datetime.now().strftime(\"%Y%m%d%H%M%s\")\n",
    "dbutils.fs.mv(filePath, archive_filepath)\n",
    "print(archive_filepath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 432430608335461,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_layer_transaction_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

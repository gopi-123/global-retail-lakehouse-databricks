{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89883948-6eb5-45ab-b5cf-17d4735525bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Customer Data into Bronze Layer in Databricks\n",
    "\n",
    "1. Create a new notebook named `Bronze_layer_Customer_load`.\n",
    "2. Load the customer CSV file from the specified path into a DataFrame.\n",
    "3. Add an `ingestion_timestamp` column to the DataFrame.\n",
    "4. Save the DataFrame as a Delta table named `bronze_customer` in the `globalretail_bronze` database.\n",
    "5. Move the processed CSV file to an archive folder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c291e8-af67-4dfd-965f-28723f5bf91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads a CSV file from the specified DBFS path into a Spark DataFrame.\n",
    "\n",
    "Parameters:\n",
    "- filePath: str\n",
    "    The path to the CSV file in Databricks File System (DBFS).\n",
    "- header: bool, default=True\n",
    "    Indicates whether the first line of the file contains column names.\n",
    "- inferSchema: bool, default=True\n",
    "    If True, automatically infers the data types of each column.\n",
    "\n",
    "Returns:\n",
    "- df: pyspark.sql.DataFrame\n",
    "    The resulting Spark DataFrame containing the CSV data.\n",
    "\"\"\"\n",
    "\n",
    "filePath = \"dbfs:/FileStore/GlobalRetail/bronze_layer/customer_data/customer.csv\"\n",
    "df = spark.read.csv(filePath, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5311dbc5-c59f-4ce5-8ec2-9c0cd5ed04ad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766513746455}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adds an 'ingestion_timestamp' column with the current timestamp to the DataFrame 'df'\n",
    "and displays the resulting DataFrame.\n",
    "\n",
    "Note: df.withColumn returns a new DataFrame; the original 'df' remains unchanged.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df_new = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "display(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33521142-3c30-43d2-b8c0-fedfb501fc8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Appends the DataFrame 'df_new' to the 'bronze_customer' Delta table in the 'globalretail_bronze' schema.\n",
    "Uses Delta format for ACID transactions, scalable metadata handling, and unified streaming/batch data processing.\n",
    "\"\"\"\n",
    "spark.sql(\"use globalretail_bronze\")\n",
    "df_new.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b17729-4a27-4b8d-a270-6752b75477e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from bronze_customer limit 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf58dfe-23d0-4d77-8418-d9657f0325de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Moves the customer data file to an archive folder in DBFS with a unique timestamped name.\n",
    "\n",
    "Archiving the ingested file ensures data lineage, auditability, and recovery by preserving the original raw data.\n",
    "This prevents accidental reprocessing, supports compliance, and enables troubleshooting or re-ingestion if needed.\n",
    "\n",
    "Catches FileNotFoundException if the source file does not exist.\n",
    "Prints the archive file path for logging.\n",
    "\"\"\"\n",
    "import datetime\n",
    "archive_folder = \"dbfs:/FileStore/GlobalRetail/bronze_layer/customer_data/archive/\"\n",
    "archive_filepath = archive_folder +'_'+datetime.datetime.now().strftime(\"%Y%m%d%H%M%s\")\n",
    "dbutils.fs.mv(filePath, archive_filepath)\n",
    "print(archive_filepath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 432430608335461,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_layer_Customer_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
